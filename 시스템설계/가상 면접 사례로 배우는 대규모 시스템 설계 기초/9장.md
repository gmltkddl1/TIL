# 웹 크롤러

## 요구사항 
- URL 집합이 주어지면 해당 URL의 가르키는 모든 웹페이지를 다운로드한다.
- 다운 받은 페이지에서 URL을 추출한다.
- 해당 URL을 목록에 추가하고 위 과정을 반복한다.
- 한달에 10억개의 웹페이지를 다운로드해야한다.
- 중복 페이지는 무시해야한다.
- 데이터 보관기간은 5년
- 웹페이지의 수정이나 추가도 반영해야한다.

## 상세구현
- BFS를 이용한 URL 서치
- 예의(너무많은 재방문 x)를 위해 큐와 도메인을 매핑시킨다 안정해쉬 사용가능
- 우선순위에 특정 큐를 많이 선택되게하여 갱신 빈도 등을 조절한다.
- 중복은 해시나 체크섬을 이용해서 쉽게 체크 가능
